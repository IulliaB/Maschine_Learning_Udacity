{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Summary\n",
    "\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives.\n",
    "\n",
    "The objective of this project is to build a Person of Interest identifier based on financial and email data made public as a result of the Enron scandal. A Person of Interest (POI) is an individual who was indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity. \n",
    "\n",
    "In trying to acomplish the objective of tis project, Machine Learning will be used. Machine Learning algorithms are able to identify patterns in data and build models ,based on the analyzed dataset, that can be applied to new datsets in order to predict future events. More specifically, the project will be based on Supervised Learning which is a Maschine Learning task that is able to learn a function that can map an input to an output based on example input-output pairs. Therefore, the supervised learning algorithm is able to analyze the training data and produce an inferred function, that can be used to match new examples.\n",
    "\n",
    "The project consists of six major steps:\n",
    "1. Data Overview\n",
    "2. Feature Selection and Engineering\n",
    "3. Algorithm Selection\n",
    "4. Parameters tunning\n",
    "5. Analysis Validation\n",
    "6. Evaluation\n",
    "\n",
    "\n",
    "# 1. Data Overview\n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "The first step in developing the project was to have an overview of the data. Therfore, an exploratory data analysis was perform in order to determine the size of the data, number of features, number of POIs etc. The results of the analysis show:\n",
    "1. The data set comprises 146 entries which represent the number of Enron executives whose email and financial data are present in the data set\n",
    "2. Each data set entry has 21 features\n",
    "3. There are 18 POIs in the dataset out of the total 34 POIs (not all the identified POIs in the fraud investigation work for Enron), meaning that the remaining 128 are non-POIs\n",
    "4. There are 1358 missing features in the dataset that are divided as follow:\n",
    "\n",
    "| Features                          | No. of missing values  |\n",
    "|-----------------------------------|------------------------|\n",
    "|   salary                          | 51                     |\n",
    "|   to_messages                     | 60                     |\n",
    "|   deferral_payments               | 107                    |\n",
    "|   total_payments                  | 21                     |\n",
    "|   loan_advances                   | 142                    |\n",
    "|   bonus                           | 64                     |\n",
    "|   email_address                   | 35                     |\n",
    "|   restricted_stock_deferred       | 128                    |\n",
    "|   total_stock_value               | 20                     |\n",
    "|   shared_receipt_with_poi         | 60                     |\n",
    "|   long_term_incentive             | 80                     |\n",
    "|   exercised_stock_options         | 44                     |\n",
    "|   from_messages                   | 60                     |\n",
    "|   other                           | 53                     |\n",
    "|   from_poi_to_this_person         | 60                     |\n",
    "|   from_this_person_to_poi         | 60                     |\n",
    "|   poi                             | 0                      |\n",
    "|   deferred_income                 | 97                     |\n",
    "|   expenses                        | 51                     |\n",
    "|   restricted_stock                | 36                     |\n",
    "|   director_fees                   | 129                    |\n",
    "\n",
    "\n",
    "## Outlier Investigation\n",
    "\n",
    "\n",
    "For the outlier analysis, two features were extracted from the dictionary, namely \"salary\" and \"bonus\". Subsequently they were used as input for the scatterplot below.\n",
    "\n",
    "![Outliers included](Figure_1_w_out.png)\n",
    "\n",
    "\n",
    "Looking at the scatterplot, there is one outlier that pops up imediately. After looking at the data more carefully, it has been noticed that the outlier actually represents the total value of the salaries and bonuses. therefore it has been decided to remove this value. Additionally, \"THE TRAVEL AGENCY IN THE PARK\" was another entry that was removed from the dataset because it was not considered representative and it had all the values zero, except for \"others\" column.\n",
    "The scatterplot bellow shows the situation after removing the outlier.\n",
    "\n",
    "![Outliers excluded](Figure_1_wo_out.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the scatter plot after removing the outlier,it can be seen that there are still left a few possible outliers. When analyzing the provided PDF with the financial data it can be observed that the values belong to:\n",
    "* Kenneth Lay\n",
    "* Jeffrey Skilling\n",
    "* Mark Frevert\n",
    "* John Lavorato\n",
    "\n",
    "Eventhough they have bonuses and salary higher than the rest, these \"outliers\" are not removed since they are considered valid data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Selection and Engineering\n",
    "\n",
    "## Feature Egineering\n",
    "\n",
    "After removing the outliers from the Salary vs Bonus scatter plot, two other important features were identified. Namely \"from_poi_to_this_person\" and \"from_this_person_to_poi\". Since there no strong pattern was obesrved two new features were created:\n",
    "* fraction_from_poi - represents the fraction between the number of emails from POIs and the total number of from messages\n",
    "* fraction_to_poi - represents the fraction between the number of emails to POIs and the total number of to messages\n",
    "\n",
    "The scatterplot between the two new created features can be seen below.\n",
    "![Messages to and from POIs](scaled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scatterplot some black stars can be observed. Those represent the messages exchanged between POIs. By highlighting this, it can be observed that non POIs exchanged much more messages between them than the POIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "The two newly created features \"fraction_from_poi\" and \"fraction_to_poi\" were scaled. However, further feature scaling was not performed since neither RandomForest nor AdaBoost require it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "The next step was to select the features that will be used further in the algorithm. Therefore a forests of trees was used to evaluate the importance of the features. \n",
    "1. feature exercised_stock_options (0.113846)\n",
    "2. feature total_stock_value (0.090760)\n",
    "3. feature fraction_to_poi (0.089054)\n",
    "4. feature deferred_income (0.075426)\n",
    "5. feature salary (0.066242)\n",
    "6. feature expenses (0.063815)\n",
    "7. feature total_payments (0.061640)\n",
    "8. feature bonus (0.061081)\n",
    "9. feature long_term_incentive (0.058439)\n",
    "10. feature restricted_stock (0.049589)\n",
    "11. feature shared_receipt_with_poi (0.049137)\n",
    "12. feature from_poi_to_this_person (0.045380)\n",
    "13. feature from_this_person_to_poi (0.039375)\n",
    "14. feature to_messages (0.031417)\n",
    "15. feature fraction_from_poi (0.028978)\n",
    "16. feature from_messages (0.027333)\n",
    "17. feature deferral_payments (0.027270)\n",
    "18. feature loan_advances (0.014175)\n",
    "19. feature restricted_stock_deferred (0.006530)\n",
    "20. feature director_fees (0.000512)\n",
    "\n",
    "For the next steps of the analysis it has been decided to select the top 10 features based on their importance scores. It can be observed that also the newly created feature \"fraction_to_poi\" it is included in the list, ranking 3rd in the list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Algorithm Selection\n",
    "\n",
    "In order to select the algorithm, beside accuracy, precision and recall were used for various classifiers. \n",
    "\n",
    "| Classifiers                   | Accuracy | Precision|Recall    |\n",
    "|-------------------------------|----------|----------|----------|\n",
    "|   AdaBoost                    |0.83053   |0.33333\t  |0.2710    |\n",
    "|   RandomForest                |0.86173   |0.43707   |0.1285\t |\n",
    "|   DecisionTree                |0.81053   |0.27935   |0.2665    |\n",
    "|   GassianNB                   |0.81273   |0.28766   |0.2740    |\n",
    "\n",
    "AdaBoost and Random Forest are the seleted classifiers for parameter tuning. \n",
    "\n",
    "\n",
    "# 4. Parameter Tuning\n",
    "\n",
    "Machine Learning algorithms comprises of parameters that have to be manually changed/defined by the person using the algorithm. Parameter tuning implies selecting the best parameters in order to optimize the performance of an algorithm. \n",
    "\n",
    "Two algorithms were selected for parameter tunning: AdaBoost and RandomForest since they hadthe highest accuracy and precison (> 0.3). Through parameter tuning is desired to increase the recall in order to have a value of at least 0.3.\n",
    "\n",
    "### RandamForest parameter tuning\n",
    "\n",
    "The first parameter tuning performed was the \"n-estimator\". The best perfomance was reached when n_estimator = 100. However, the increase in performance was not enough since recall < 0.3.\n",
    "\n",
    "| Parameters                   | Accuracy | Precision|Recall    |\n",
    "|------------------------------|----------|----------|----------|\n",
    "|   n-estimator = 10           |0.86173   |0.43707   |0.1285    |\n",
    "|   n-estimator = 50           |0.86467   |0.47440   |0.1390    |\n",
    "|   n-estimator = 100          |0.86567   |0.48731   |0.1440    |\n",
    "\n",
    "Since the reacall scores were far from the desired level (>0.3), the parameters for the AdaBoost clasifier will be tuned next. \n",
    "\n",
    "### AdaBoost parameter tuning\n",
    "\n",
    "It has been observed that running AdaBoost with the top 10 features had a negative impact on performance, having a recall below 0.3. Therefore it has been decided to decrease the number of features one by one. The best results were obtained with the top 6 features.\n",
    "\n",
    "Since the recall was not at the desired level, furtehr parameters tuning was performed. \n",
    "\n",
    "| Parameters                   | Accuracy | Precision|Recall    |\n",
    "|------------------------------|----------|----------|----------|\n",
    "|   n-estimator = 10           |0.85427   |0.42159   |0.2500    |\n",
    "|   n-estimator = 15           |0.85040\t  |0.41801\t |0.3110    |\n",
    "|   n-estimator = 20           |0.85200   |0.43284   |0.3545    |\n",
    "|   n-estimator = 25           |0.85807   |0.46903   |0.3805    |\n",
    "\n",
    "The recall score exceeded 0.30 when the n-estimator was 15. When increasing the value of the n_estimator parameter, the recall value increased, but the time required to run cross validation was increased over 1 minute. Therefore, the selected model is AdaBoost classifier with the n_estimator = 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analysis Validation\n",
    "\n",
    "Validation implies partitioning the dataset into two different subsets. The analysis is performed on one subeset, called the trainng set, while the validation of the analysis is performed on the other set, called the training set. \n",
    "If this partion of the data is not perfomed, and the model trains and tests on the same dataset, it leads to overfitting. \n",
    "\n",
    "# 6. Evaluation\n",
    "\n",
    "As perviously mentioned, the evaluation metrics used to assess the final model were: accuracy, recall and precision. The overview below shows the prformance of the model.\n",
    "\n",
    "| Parameters                   | Accuracy | Precision|Recall    |\n",
    "|------------------------------|----------|----------|----------|\n",
    "|   AdaBoost                   |0.85040   | 0.41801  |0.3110    |\n",
    "\n",
    "The accuracy, represents the number of correct predictions out of the total predicions made by the model. In this case, the model predicted correctly in aprox. 85% of the cases. The precision shows the percentage of item considered relevant. Therfore, considering all the items identified as POIs, aprox. 42% of the were actually POIs. A 31% recall, means that from all the POIs from the dataset the model correctly identified 31% of them. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
