{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Summary\n",
    "\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives.\n",
    "\n",
    "The objective of this project is to build a Person of Interest identifier based on financial and email data made public as a result of the Enron scandal. A Person of Interest (POI) is an individual who was indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity. \n",
    "\n",
    "In trying to acomplish the objective of tis project, Machine Learning will be used. Machine Learning algorithms are able to identify patterns in data and build models ,based on the analyzed dataset, that can be applied to new datsets in order to predict future events. More specifically, the project will be based on Supervised Learning which is a Maschine Learning task that is able to learn a function that can map an input to an output based on example input-output pairs. Therefore, the supervised learning algorithm is able to analyze the training data and produce an inferred function, that can be used to match new examples.\n",
    "\n",
    "The project consists of six major steps:\n",
    "1. Data Overview\n",
    "2. Feature Selection and Engineering\n",
    "3. Algorithm Selection\n",
    "4. Parameters tunning\n",
    "5. Analysis Validation\n",
    "6. Evaluation\n",
    "\n",
    "\n",
    "# 1. Data Overview\n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "The first step in developing the project was to have an overview of the data. Therfore, an exploratory data analysis was perform in order to determine the size of the data, number of features, number of POIs etc. The results of the analysis show:\n",
    "1. The data set comprises 146 entries which represent the number of Enron executives whose email and financial data are present in the data set\n",
    "2. Each data set entry has 21 features\n",
    "3. There are 18 POIs in the dataset out of the total 34 POIs (not all the identified POIs in the fraud investigation work for Enron), meaning that the remaining 128 are non-POIs\n",
    "4. There are 1358 missing features in the dataset that are divided as follow:\n",
    "\n",
    "| Features                          | No. of missing values  |\n",
    "|-----------------------------------|------------------------|\n",
    "|   salary                          | 51                     |\n",
    "|   to_messages                     | 60                     |\n",
    "|   deferral_payments               | 107                    |\n",
    "|   total_payments                  | 21                     |\n",
    "|   loan_advances                   | 142                    |\n",
    "|   bonus                           | 64                     |\n",
    "|   email_address                   | 35                     |\n",
    "|   restricted_stock_deferred       | 128                    |\n",
    "|   total_stock_value               | 20                     |\n",
    "|   shared_receipt_with_poi         | 60                     |\n",
    "|   long_term_incentive             | 80                     |\n",
    "|   exercised_stock_options         | 44                     |\n",
    "|   from_messages                   | 60                     |\n",
    "|   other                           | 53                     |\n",
    "|   from_poi_to_this_person         | 60                     |\n",
    "|   from_this_person_to_poi         | 60                     |\n",
    "|   poi                             | 0                      |\n",
    "|   deferred_income                 | 97                     |\n",
    "|   expenses                        | 51                     |\n",
    "|   restricted_stock                | 36                     |\n",
    "|   director_fees                   | 129                    |\n",
    "\n",
    "\n",
    "## Outlier Investigation\n",
    "\n",
    "\n",
    "For the outlier analysis, two features were extracted from the dictionary, namely \"salary\" and \"bonus\". Subsequently they were used as input for the scatterplot below.\n",
    "\n",
    "![Outliers included](Figure_1_w_out.png)\n",
    "\n",
    "\n",
    "Looking at the scatterplot, there is one outlier that pops up imediately. After looking at the data more carefully, it has been noticed that the outlier actually represents the total value of the salaries and bonuses. therefore it has been decided to remove this value. Additionally, \"THE TRAVEL AGENCY IN THE PARK\" was another entry that was removed from the dataset because it was not considered representative and it had all the values zero, except for \"others\" column.\n",
    "The scatterplot bellow shows the situation after removing the outlier.\n",
    "\n",
    "![Outliers excluded](Figure_1_wo_out.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the scatter plot after removing the outlier,it can be seen that there are still left a few possible outliers. When analyzing the provided PDF with the financial data it can be observed that the values belong to:\n",
    "* Kenneth Lay\n",
    "* Jeffrey Skilling\n",
    "* Mark Frevert\n",
    "* John Lavorato\n",
    "\n",
    "Eventhough they have bonuses and salary higher than the rest, these \"outliers\" are not removed since they are considered valid data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Selection and Engineering\n",
    "\n",
    "## Feature Egineering\n",
    "\n",
    "After removing the outliers from the Salary vs Bonus scatter plot, two other important features were identified. Namely \"from_poi_to_this_person\" and \"from_this_person_to_poi\". Since there no strong pattern was obesrved two new features were created:\n",
    "* fraction_from_poi - represents the fraction between the number of emails from POIs and the total number of from messages\n",
    "* farction_to_poi - represents the fraction between the number of emails to POIs and the total number of to messages\n",
    "\n",
    "The scatterplot between the two new created features can be seen below.\n",
    "![Messages to and from POIs](scaled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scatterplot some black stars can be observed. Those represent the messages exchanged between POIs. By highlighting this, it can be observed that non POIs exchanged much more messages between them than the POIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "The next step was to select the features that will be used further in the algorithm. Therefore a forests of trees was used to evaluate the importance of the features. The top ten features based on their importance was selected. Moreover, some features considered importantant and relevant for the analysis were manually added. The final list of selected features is:\n",
    "1. total_payments (0.113846)\n",
    "2. restricted_stock_deferred (0.090760)\n",
    "3. fraction_from_poi (0.089054)\n",
    "4. director_fees (0.075426)\n",
    "5. poi (0.066242)\n",
    "6. total_stock_value (0.063815)\n",
    "7. deferral_payments (0.061640)\n",
    "8. exercised_stock_options (0.061081)\n",
    "9. deferred_income (0.058439)\n",
    "10. bonus (0.049589)\n",
    "11. salary (0.031417)\n",
    "12. expenses (0.014175)\n",
    "13. from_poi_to_this_person (0.000512)\n",
    "14. fraction_to_poi\n",
    "\n",
    "From the engineered features, fraction_from_poi was included in the selected list of fearures since it ranked third based on the importance of the features. Initially, the econd ingineered feature, fraction_to_poi, was not included in the list because it had a very low (the lowest) importance score. However, by adding the feature to the list, the accuracy using the AdaBoost classifier improved from 0.819 to 0.847\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Algorithm Selection\n",
    "\n",
    "In order to select the algorithm, beside accuracy, precision and recall were used for various classifiers. \n",
    "\n",
    "| Classifiers                   | Accuracy | Precision|Recall    |\n",
    "|-------------------------------|----------|----------|----------|\n",
    "|   AdaBoost                    |0.83053   |0.33333\t  |0.2710    |\n",
    "|   RandomForest                |0.86173   |0.43707   |0.1285\t |\n",
    "|   DecisionTree                |0.81053   |0.27935   |0.2665    |\n",
    "|   GassianNB                   |0.81273   |0.28766   |0.2740    |\n",
    "\n",
    "AdaBoost and Random Forest are the seleted classifiers for parameter tuning. \n",
    "\n",
    "\n",
    "# 4. Parameter Tuning\n",
    "\n",
    "Machine Learning algorithms comprises of parameters that have to be manually changed/defined by the person using the algorithm. Parameter tuning implies selecting the best parameters in order to optimize the performance of an algorithm. \n",
    "\n",
    "Two algorithms were selected for parameter tunning: AdaBoost and RandomForest since they hadthe highest accuracy and precison (> 0.3). Through parameter tuning is desired to increase the recall in order to have a value of at least 0.3.\n",
    "\n",
    "### RandamForest parameter tuning\n",
    "\n",
    "The first parameter tuning performed was the \"n-estimator\". The best perfomance was reached when n_estimator = 100. However, the increase in performance was not enough since recall < 0.3.\n",
    "\n",
    "| Parameters                   | Accuracy | Precision|Recall    |\n",
    "|------------------------------|----------|----------|----------|\n",
    "|   n-estimator = 10           |0.86173   |0.43707   |0.1285    |\n",
    "|   n-estimator = 50           |0.86467   |0.47440   |0.1390    |\n",
    "|   n-estimator = 100          |0.86567   |0.48731   |0.1440    |\n",
    "\n",
    "Since the reacall scores were far from the desired level (>0.3), the parameters for the AdaBoost clasifier will be tuned next. \n",
    "\n",
    "### AdaBoost parameter tuning\n",
    "\n",
    "The first parameter tuning performed for AdaBoost was changing the algorithm from SAMME.R to SAMME.\n",
    "\n",
    "| Parameters                   | Accuracy | Precision|Recall    |\n",
    "|------------------------------|----------|----------|----------|\n",
    "|   algorithm = SAMME.R        |0.83027   |0.33231   |0.2705    |\n",
    "|   algorithm = SAMME          |0.84280   |0.36460   |0.2410    |\n",
    "\n",
    "In can be observed that whil the precision increased, the recall decreased. Therefore the n-estimator parameter was selected for tuning. The default value for this parameter is 50, and the value was gradually incraesed in oredr to obtain the desired recall level.\n",
    "\n",
    "| Parameters                   | Accuracy | Precision|Recall    |\n",
    "|------------------------------|----------|----------|----------|\n",
    "|   n-estimator = 50           |0.83027   |0.33231   |0.2705    |\n",
    "|   n-estimator = 100          |0.8418\t  |0.3769\t |0.2855    |\n",
    "|   n-estimator = 150          |0.846\t  |0.39427\t |0.289     |\n",
    "|   n-estimator = 200          |0.84687\t  |0.39905\t |0.2935    |\n",
    "|   n-estimator = 300          |0.84573   |0.39547   |0.2970    |\n",
    "|   n-estimator = 600          |0.84573   | 0.39671  |0.3015    |\n",
    "\n",
    "The recall score reached 0.30 when the n-estimator was 600. Therefore, this is the selected model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analysis Validation\n",
    "\n",
    "Validation implies partitioning the dataset into two different subsets. The analysis is performed on one subeset, called the trainng set, while the validation of the analysis is performed on the other set, called the training set. \n",
    "If this partion of the data is not perfomed, and the model trains and tests on the same dataset, it leads to overfitting. \n",
    "\n",
    "# 6. Evaluation\n",
    "\n",
    "As perviously mentioned, the evaluation metrics used to assess the final model were: accuracy, recall and precision. The overview below shows the prformance of the model.\n",
    "\n",
    "| Parameters                   | Accuracy | Precision|Recall    |\n",
    "|------------------------------|----------|----------|----------|\n",
    "|   AdaBoost                   |0.84573   | 0.39671  |0.3015    |\n",
    "\n",
    "The accuracy, represents the number of correct predictions out of the total predicions made by the model. In this case, the model predicted correctly in aprox. 85% of the cases. The precision shows the percentage of item considered relevant. Therfore, considering all the items identified as POIs, aprox. 40% of the were actually POIs. A 30% recall, means that from all the POIs from the dataset the model correctly identified 30% of them. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
